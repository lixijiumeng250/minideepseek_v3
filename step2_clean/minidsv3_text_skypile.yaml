# Process config example including:
#   - all global arguments
#   - all ops and their arguments

# global parameters
project_name: 'all'                                         # project name for distinguish your configs
dataset_path: '/mnt/data/workspace/sy_transformers/minideepseek/step1_data/basic_clean/test'                       # path to your dataset directory or file with weights(0.0-1.0), 1.0 as default.
                                                            # accepted format: 'weight1(optional) dataset1-path weight2(optional) dataset2-path'
export_path: '/mnt/data/workspace/sy_transformers/minideepseek/djclean_data/djed_skypile.jsonl'                # path to processed result dataset. Supported suffixes include ['jsonl', 'json', 'parquet']
export_shard_size: 100000000                                        # 100MB shard size of exported dataset in Byte. In default, it's 0, which means export the whole dataset into only one file. If it's set a positive number, the exported dataset will be split into several dataset shards, and the max size of each shard won't larger than the export_shard_size
export_in_parallel: false                                   # whether to export the result dataset in parallel to a single file, which usually takes less time. It only works when export_shard_size is 0, and its default number of processes is the same as the argument np. **Notice**: If it's True, sometimes exporting in parallel might require much more time due to the IO blocking, especially for very large datasets. When this happens, False is a better choice, although it takes more time.
np: 32                                                       # number of subprocess to process your dataset
text_keys: 'text'                                        # the key name of field where the sample texts to be processed, e.g., `text`, `instruction`, `output`, ...
                                                            # Note: currently, we support specify only ONE key for each op, for cases requiring multiple keys, users can specify the op multiple times. We will only use the first key of `text_keys` when you set multiple keys.
suffixes: []                                                # the suffix of files that will be read. For example: '.txt', 'txt' or ['txt', '.pdf', 'docx']
use_cache: false                                             # whether to use the cache management of Hugging Face datasets. It might take up lots of disk space when using cache
ds_cache_dir: /mnt/data/workspace/sy_transformers/minideepseek/cache/dj_cache  # cache dir for Hugging Face datasets. 使用大容量磁盘避免空间不足
use_checkpoint: false                                       # whether to use the checkpoint management to save the latest version of dataset to work dir when processing. Rerun the same config will reload the checkpoint and skip ops before it. Cache will be disabled when using checkpoint. If args of ops before the checkpoint are changed, all ops will be rerun from the beginning.
temp_dir: /mnt/data/workspace/sy_transformers/minideepseek/cache/dj_temp      # the path to the temp directory to store intermediate caches. 使用大容量磁盘避免空间不足
open_tracer: true                                          # whether to open the tracer to trace the changes during process. It might take more time when opening tracer
op_list_to_trace: []                                        # only ops in this list will be traced by tracer. If it's empty, all ops will be traced. Only available when tracer is opened.
trace_num: 10                                               # number of samples to show the differences between datasets before and after each op. Only available when tracer is opened.
op_fusion: true                                            # whether to fuse operators that share the same intermediate variables automatically. Op fusion might reduce the memory requirements slightly but speed up the whole process.
cache_compress: zstd                                        # the compression method of the cache file, which can be specified in ['gzip', 'zstd', 'lz4']. If this parameter is None, the cache file will not be compressed. We recommend you turn on this argument when your input dataset is larger than tens of GB and your disk space is not enough.

# eoc_special_token: '<|__dj__eoc|>'                          # the special token that represents the end of a chunk in the text. In default, it's "<|__dj__eoc|>". You can specify your own special token according to your input dataset.

# only for data analysis
save_stats_in_one_file: true                               # whether to store all stats result into one file

# process schedule: a list of several process operators with their arguments
process:
  # Mapper ops. Most of these ops need no arguments.
  - chinese_convert_mapper:                                 # convert Chinese between Traditional Chinese, Simplified Chinese and Japanese Kanji.
      mode: 't2s'                                             # choose the mode to convert Chinese: ['s2t', 't2s', 's2tw', 'tw2s', 's2hk', 'hk2s', 's2twp', 'tw2sp', 't2tw', 'tw2t', 'hk2t', 't2hk', 't2jp', 'jp2t']
  - clean_email_mapper:                                     # remove emails from text.
  - clean_html_mapper:                                      # remove html formats form text.
  - clean_ip_mapper:                                        # remove ip addresses from text.
  - clean_links_mapper:                                     # remove web links from text.
  - clean_copyright_mapper:                                 # remove copyright comments.
  - expand_macro_mapper:                                    # expand macro definitions in Latex text.
#  - fix_unicode_mapper:                                     # fix unicode errors in text.

#   - nlpcda_zh_mapper:                                       # simply augment texts in Chinese based on the nlpaug library
#       sequential: false                                       # whether combine all augmentation methods to a sequence. If it's True, a sample will be augmented by all opened augmentation methods sequentially. If it's False, each opened augmentation method would generate its augmented samples independently.
#       aug_num: 1                                              # number of augmented samples to be generated. If `sequential` is True, there will be total aug_num augmented samples generated. If it's False, there will be (aug_num * #opened_aug_method) augmented samples generated.
#       keep_original_sample: true                              # whether to keep the original sample. If it's set to False, there will be only generated texts in the final datasets and the original texts will be removed. It's True in default.
#       replace_similar_word: false                             # whether to open the augmentation method of replacing random words with their similar words in the original texts. e.g. "这里一共有5种不同的数据增强方法" --> "这边一共有5种不同的数据增强方法"
#       replace_homophone_char: false                           # whether to open the augmentation method of replacing random characters with their homophones in the original texts. e.g. "这里一共有5种不同的数据增强方法" --> "这里一共有5种不同的濖据增强方法"
#       delete_random_char: false                               # whether to open the augmentation method of deleting random characters from the original texts. e.g. "这里一共有5种不同的数据增强方法" --> "这里一共有5种不同的数据增强"
#       swap_random_char: false                                 # whether to open the augmentation method of swapping random contiguous characters in the original texts. e.g. "这里一共有5种不同的数据增强方法" --> "这里一共有5种不同的数据强增方法"
#       replace_equivalent_num: false                           # whether to open the augmentation method of replacing random numbers with their equivalent representations in the original texts. **Notice**: Only for numbers for now. e.g. "这里一共有5种不同的数据增强方法" --> "这里一共有伍种不同的数据增强方法"
  - punctuation_normalization_mapper:                       # normalize unicode punctuations to English punctuations.

#  - remove_repeat_sentences_mapper:                         # remove repeat sentences in text samples.
#      lowercase: false                                        # whether to convert sample text to lower case
#      ignore_special_character: true                          # whether to ignore special characters when judging repeated sentences. Special characters are all characters except Chinese characters, letters and numbers
#      min_repeat_sentence_length: 2                           # sentences shorter than this length will not be deduplicated. If ignore_special_character is set to True, then special characters are not included in this length
  - remove_specific_chars_mapper:                           # remove characters specified by users
      chars_to_remove: '◆●■►▼▲▴∆▻▷❖♡□'                        # a string or a list including those characters that need to be removed
  - whitespace_normalization_mapper:                        # normalize different kinds of whitespaces to English whitespace.

    # Filter ops
  # 困惑度筛选，考虑在困惑度上进行宽松的筛选、给了比较大的困惑度
  - perplexity_filter: 
      lang: zh                                                
      max_ppl: 2500              
      
    # 单样本文本字符数
  - text_length_filter:
      max_len: 70000 # 极值与95%分位数差异太大、对文本来说，取极值的1/2

    # 平均行长度
  - average_line_length_filter:
      min_len: 10 # 最小值的两倍
      max_len: 80000 # 极值与95%分位数差异太大、取极值的1/2

    # 最长行长度
  - maximum_line_length_filter:
      max_len: 80000  # 取极值的1/2

    # 字符重复率
  - character_repetition_filter:
      max_ratio: 0.9  # 极值与95%分位数差异太大、取极值的1/2

    # 特殊字符比例
  - alphanumeric_filter:
      tokenization: false
      min_ratio: 0.0  # 不设下限
      max_ratio: 0.95  # 极值太大、取极值的1/2

  # Deduplicator ops
#  - document_simhash_deduplicator:                          # deduplicate text samples using SimHash-LSH method
#      tokenization: space                                     # tokenization method for text. One of [space, punctuation, character]
#      window_size: 2                                          # 相对宽松的去重
#      num_blocks: 8                                           # number of blocks in SimHash computing
#      hamming_distance: 6                                     # the max hamming distance to regard 2 samples as similar enough pair. Should be less than num_blocks always
#      lowercase: true                                         # whether to convert text to lower case
#      ignore_pattern: null                                    # whether to ignore sub-strings with specific pattern when computing simhash.

